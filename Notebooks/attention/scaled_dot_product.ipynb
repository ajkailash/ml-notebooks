{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c59b86-7c45-4ac8-923c-9bcabfffe5d8",
   "metadata": {},
   "source": [
    "For simplicity we're are gonna use the following\n",
    "- Use a single sentence, not a full corpus.\n",
    "- Use a small embedding dimension (3). This allows use examine individual vectors without filling the entire page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64f7c79-d327-4014-ad80-ccb5f3b0c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab042df7-551e-4561-93d1-ce5220bcb7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f2cf190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7460e41-137c-4024-bc8d-76a0a7365c08",
   "metadata": {},
   "source": [
    "### Embedding a input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12064c53-ba7f-43ed-95aa-1b1760d303bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Life is short, eat dessert first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "203fd2ed-f8c8-4415-ae5e-fc384ac670d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab is restricted to the words in the sentence :)\n",
    "words = sorted([s for s in sentence.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39456a5-bb18-4134-94af-c9c981b5c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {w:idx for idx, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eee6ca3-782d-4bfe-9fdb-b82ebc452206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the word_to_idx dict to assign integer index to the word\n",
    "sentence_int = torch.tensor([word_to_idx[s] for s in sentence.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d7c8e8-8617-4c0e-bebd-dcc5dc607d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embed = torch.nn.Embedding(vocab_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173b9a35-afec-479c-9729-2b025468944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentence = embed(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14aba66-f6c7-473c-a810-73d090c7ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035],\n",
       "        [ 0.1794,  1.8951,  0.4954],\n",
       "        [ 0.2692, -0.0770, -1.0205],\n",
       "        [-0.2196, -0.3792,  0.7671],\n",
       "        [-0.5880,  0.3486,  0.6603],\n",
       "        [-1.1925,  0.6984, -1.4097]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57219c7d-f837-4091-b5fb-d75321023b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78673034-7d7b-4854-b9de-d566fb3c4d89",
   "metadata": {},
   "source": [
    "### Defining weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d831845-be44-49ff-b4cf-45c3603b346a",
   "metadata": {},
   "source": [
    "- Usually a linear layer is used to create the Query, Key and Value matrices, here we are gonna use nn.Parameter layer\n",
    "- The embedding dim of the value matrix need not be the same size of Query and Value, it can be arbitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafef055-d481-4e62-bcc0-1a79042fbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_q, d_k, d_v = 2, 2, 4\n",
    "d = embedded_sentence.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51583cf0-c1f3-4da9-b6ca-f805b91424da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = torch.nn.Parameter(torch.randn(d, d_q))\n",
    "w_key = torch.nn.Parameter(torch.randn(d, d_k))\n",
    "w_value = torch.nn.Parameter(torch.randn(d, d_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4badaf52-2e7d-4b68-9134-c39d514d231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = embedded_sentence @ w_key\n",
    "values = embedded_sentence @ w_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9083b101-af48-4783-ad01-b040c1bfd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compute the attention weights for one single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284cc530-f297-4fbc-b1e4-da3b3af5a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = embedded_sentence[1]\n",
    "query_1 = x_1[None, :] @ w_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde99b88-2960-4ae5-a7b5-7f63a430c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_1 = query_1 @ keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2150e17a-63f3-43d4-aa6c-b8cff9ce10e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6150,  2.4277, -0.9584,  0.2260,  1.2082,  0.5242]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87ac635c-dfe8-479d-a111-55d59156c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(omega_1 / d_k ** 0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56c3f38b-6137-4f92-ae65-b0a82c23d968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfb3e7a1-6072-43e5-b4ee-fc2827ce7a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d54961-ec90-4ba7-b35a-c825cbb409cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector_1 = attention_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a56e3c4-f993-4133-b694-e31f990ada68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1c901-4a8e-4289-a026-ca5ce3d377d9",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "- Now let's summarize the previous section in a single SelfAttention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39d64956-c527-4d3e-aaf5-de4c7b786bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.w_query = nn.Parameter(torch.randn(d_in, d_out_kq))\n",
    "        self.w_key = nn.Parameter(torch.randn(d_in, d_out_kq))\n",
    "        self.w_value = nn.Parameter(torch.randn(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.w_query\n",
    "        keyes = x @ self.w_key\n",
    "        values = x @ self.w_value\n",
    "\n",
    "        attn_scores = queries @ keyes.T\n",
    "\n",
    "        attn_weights = torch.softmax((attn_scores / self.d_out_kq ** 0.5), dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a69b026-84de-4375-9ce2-6cabd19782a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = SelfAttention(3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca99d913-448a-46a8-8fa5-6cc19f1cf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_out = self_attn(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46c17cb1-e770-4cfa-94fe-ac8c96e236b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(sa_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911a053-d782-42a0-bd89-f389a82cf56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
