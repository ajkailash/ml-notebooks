{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b2009f0-0023-407f-b47a-fa48887ca9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from helper import plot_quantization_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446dbbf6-39aa-48fc-a3d5-dd65fec96f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formula\n",
    "# r = s(q - z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278d9d69-aab1-4b7f-a064-30f929b98410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(input, weight, scales, bias):\n",
    "    casted_weight = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weight) * scales\n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87cfd77-8410-4f2a-89c2-a4bb0435630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weights\", torch.randint(-128, 128, (out_features, in_features), dtype=torch.int8))\n",
    "\n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        #rmax / qmax\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "        scales = torch.max(torch.abs(weights), dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.scales = scales\n",
    "        self.int8_weights = int8_weights\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.quantize()\n",
    "        return w8_a16_forward(input, self.int8_weights, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa0bb6c8-0219-441a-ad55-c0358ea48aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(10, 2)\n",
    "        self.linear_1 = nn.Linear(2, 4)\n",
    "        self.linear_2 = nn.Linear(4, 6, bias=False)\n",
    "        self.lm_head = nn.Linear(6, 2)\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f090037-53c9-400c-b80f-e39a2dd9f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f5644b2-8a62-45ac-b4f3-cbf718eb888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name_to_exclude = [\"lm_head\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5417e4f9-e7e4-43c9-b475-6edca1652bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "            if old_bias is not None:\n",
    "                getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            replace_linear_with_target(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a768fa3c-c6d6-4ab2-a5de-0f90146bad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target(model, W8A16LinearLayer, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a413e8a-f31f-4e8d-8231-c953819be3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyModel(\n",
       "  (embedding): Embedding(10, 2)\n",
       "  (linear_1): W8A16LinearLayer()\n",
       "  (linear_2): W8A16LinearLayer()\n",
       "  (lm_head): Linear(in_features=6, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef13d313-88d3-4b8e-93b4-c71b349ecc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target_quantize(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weights = child.weight\n",
    "            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "            getattr(module, name).quantize(old_weights)\n",
    "            if old_bias is not None:\n",
    "                getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            replace_linear_with_target_quantize(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54f53485-7505-4a72-84fe-b8bce599c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "963d999a-3af2-440d-99b0-57b0e95104a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_quantize(model_2, W8A16LinearLayer, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcb8b875-d421-4347-9800-cf003de4ac68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyModel(\n",
       "  (embedding): Embedding(10, 2)\n",
       "  (linear_1): W8A16LinearLayer()\n",
       "  (linear_2): W8A16LinearLayer()\n",
       "  (lm_head): Linear(in_features=6, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5058edaf-5098-4e83-be90-072714864ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 127,   61],\n",
       "        [-127,   48],\n",
       "        [ 127,  -77],\n",
       "        [  30,  127]], dtype=torch.int8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.linear_1.int8_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e7a4c-a298-47aa-a667-07be2039d363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
